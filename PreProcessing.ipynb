{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f1988-6ffe-43c2-811b-8a3befaacd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google.cloud import storage\n",
    "import nltk\n",
    "from langchain.document_loaders import GCSFileLoader, GCSDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "PROJECT_ID = \n",
    "LOCATION = \n",
    "BUCKET_NAME = \n",
    "BUCKET_URI = \n",
    "SERVICE_ACCOUNT=\n",
    "prefix=\n",
    "aip.init(project=PROJECT_ID, location=LOCATION)\n",
    "gcs = storage.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b168cea-4119-4f67-bcab-ad9abc440478",
   "metadata": {},
   "source": [
    "## Create chunks of text data using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ff6ee-9461-4d8f-8435-a4f7d8b59d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GCSDirectoryLoader(project_name=PROJECT_ID, bucket=BUCKET_NAME, prefix=prefix)\n",
    "documents=loader.load()\n",
    "r_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators = [\"\\n\",\" \",\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "r_docs = r_text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad72f78-00e8-4cab-9e91-31028498555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = []\n",
    "for s in r_docs:\n",
    "    # print(s.metadata['source'])\n",
    "    r = {\"metadata\": s.metadata['source'], \"content\": s.page_content}\n",
    "    chunked.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079dac5-8b17-41ee-9839-6127e504891b",
   "metadata": {},
   "source": [
    "## Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1846b-8307-4099-95d7-9a8f9c1b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "def embed_text(\n",
    "    texts: List[str] = [\"Test 1 \", \"test 2\"],\n",
    "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
    "    model_name: str = \"text-embedding-004\",\n",
    "    dimensionality: Optional[int] = 256,\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "    return [embedding.values for embedding in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050aea2-0222-495d-b7c4-fa149646fbf7",
   "metadata": {},
   "source": [
    "## Generating Summary of table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace2dac-7c0f-4840-88fe-b2a248cb4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "blobs_specific = list(bucket.list_blobs(prefix=prefix))\n",
    "blob_names = [blob.name for blob in blobs_specific]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a9947-42f2-4aa9-9f65-50a551c6b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "def llm_prediction(prompt:str):\n",
    "    \n",
    "    model = GenerativeModel(model_name=\"gemini-1.0-pro-002\",generation_config=GenerationConfig(\n",
    "                temperature=0.0,\n",
    "                top_p=1,\n",
    "                top_k=10,\n",
    "                candidate_count=1,\n",
    "                max_output_tokens=200,\n",
    "                stop_sequences=[\"STOP!\"],\n",
    "            ))\n",
    "    \n",
    "    prompt_text = f\"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "    Give a concise summary of the table or text. Table or text chunk: {prompt} \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt_text).text\n",
    "    except Exception as e:\n",
    "        return \"NA\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6923e18-8a7f-40b2-9aae-f944c9b9a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summary = {\n",
    "    \"metadata\": [],\n",
    "    \"content\": [],\n",
    "    \"summary\": [],\n",
    "}\n",
    "for files in blob_names:\n",
    "    if files[-1]!=\"/\":\n",
    "        print(\"\\n *** Generating summary of filename: \",files)\n",
    "        blob = bucket.get_blob(files)\n",
    "        downloaded_blob = blob.download_as_string()\n",
    "        result = llm_prediction(downloaded_blob)\n",
    "        if result == \"NA\":\n",
    "            print(\"No text found...\")\n",
    "            continue\n",
    "        table_summary[\"metadata\"].append(files)\n",
    "        table_summary[\"content\"].append(f\"{downloaded_blob}\")\n",
    "        table_summary[\"summary\"].append(f\"{result}\")\n",
    "        print(f\"\\n *** Summary of the above table in {files} is done..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e63606-19f1-467d-8ba1-55b304a9450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summary_embedding.to_csv(\"table_embedding.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae171ad-cb47-4d2d-94ac-a4784965b32b",
   "metadata": {},
   "source": [
    "## Generate embeddings of chunks created by Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54b61d-99f7-43f1-b0a7-73bc4e11452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "temp_chunked = chunked\n",
    "for i in range(0, len(temp_chunked), batch_size):\n",
    "    request = [x[\"content\"] for x in temp_chunked[i : i + batch_size]]\n",
    "    response = embed_text(request)\n",
    "    # Store the retrieved vector embeddings for each chunk back.\n",
    "    for x, e in zip(chunked[i : i + batch_size], response):\n",
    "        x[\"embedding\"] = e\n",
    "\n",
    "# Store the generated embeddings in a pandas dataframe.\n",
    "conf_embeddings = pd.DataFrame(temp_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2e49b-9719-4c84-98ee-0be04da6e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_embeddings.to_csv(\"research_embedding.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7e49c-41f8-4dc6-ba52-b889a98ac7ea",
   "metadata": {},
   "source": [
    "## Generate embeddings of Table summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a6ac0-f66e-4235-981e-32a234afe937",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_table_embedding = table_summary_embedding\n",
    "request = temp_table_embedding[\"summary\"]\n",
    "response = embed_text(request)\n",
    "temp_table_embedding[\"embedding\"]=response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c1f5d-b4d7-4b6a-9883-97dcc7edb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_table_embedding.to_csv(\"table_summary_embedding.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce78d7-8459-4483-8d02-02d6c9939242",
   "metadata": {},
   "source": [
    "## Storing the embeddings in chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f44bb-551b-4509-aead-b8e20dc99835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4c8b6-212e-4973-a4c1-23c966229fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"my_db\")\n",
    "collection = chroma_client.get_or_create_collection(\"research_paper_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df827e18-f7ec-4d51-b96e-5173fae2476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_=[\"metadata\",\"content\",\"embedding\"]\n",
    "df_1 = pd.read_csv(\"research_embedding.csv\",usecols=columns_)\n",
    "\n",
    "df_2 = pd.read_csv(\"research_table_summary_embedding.csv\",usecols=columns_)\n",
    "df_3= pd.concat([df_1, df_2], ignore_index=True)\n",
    "df_3.to_csv(\"consolidated_embeddings.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d97819-02ab-4f5f-ad11-29eebc24e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Load sample data (a restaurant menu of items)\n",
    "with open('consolidated_embeddings.csv') as file:\n",
    "    lines = csv.reader(file)\n",
    "\n",
    "    # Store the name of the menu items in this array. In Chroma, a \"document\" is a string i.e. name, sentence, paragraph, etc.\n",
    "    documents = []\n",
    "\n",
    "    # Store the corresponding menu item IDs in this array.\n",
    "    metadatas = []\n",
    "    embeddings = []\n",
    "\n",
    "    # Each \"document\" needs a unique ID. This is like the primary key of a relational database. We'll start at 1 and increment from there.\n",
    "    ids = []\n",
    "    id = 1\n",
    "\n",
    "    # Loop thru each line and populate the 3 arrays.\n",
    "    for i, line in enumerate(lines):\n",
    "        if i==0:\n",
    "            # Skip the first row (the column headers)\n",
    "            continue\n",
    "\n",
    "        documents.append(line[1])\n",
    "        metadatas.append({\"item_id\": line[0]})\n",
    "        embeddings.append(np.concatenate(np.asarray(np.matrix(line[2]))).tolist())\n",
    "        ids.append(str(id))\n",
    "        id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83181f28-5fe8-4a07-af53-b53423f7b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
