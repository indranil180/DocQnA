{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a724b-1018-4e2c-b6b8-3730b971e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import vertexai\n",
    "import pandas as pd\n",
    "\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "PROJECT_ID =\n",
    "LOCATION = \n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "def embed_text(\n",
    "    texts: List[str],\n",
    "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
    "    model_name: str = \"text-embedding-004\",\n",
    "    dimensionality: Optional[int] = 256,\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
    "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
    "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
    "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
    "    return [embedding.values for embedding in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044be8a-8f76-4375-94fd-d58a020eac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "\n",
    "def llm_prediction(prompt:str):\n",
    "    model = GenerativeModel(model_name=\"gemini-1.0-pro-002\",generation_config=GenerationConfig(\n",
    "                temperature=0.0,\n",
    "                top_p=1,\n",
    "                top_k=10,\n",
    "                candidate_count=1,\n",
    "                max_output_tokens=250,\n",
    "                stop_sequences=[\"STOP!\"],\n",
    "            ),)\n",
    "    \n",
    "    context=retreive_context(prompt)\n",
    "    prompt_text = f\"\"\"DOCUMENT:\n",
    "    {context}\n",
    "\n",
    "     QUESTION:\n",
    "     {prompt}\n",
    "     \n",
    "     INSTRUCTIONS:\n",
    "     Answer the users QUESTION using the DOCUMENT text above.\n",
    "     Keep your answer ground in the facts of the DOCUMENT.\n",
    "     If the DOCUMENT doesnâ€™t contain the facts to answer the QUESTION return \"I dont know the answer !\" \"\"\"\n",
    "    \n",
    "    return model.generate_content(prompt_text).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba6668-dece-4f42-babc-d56453fcccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_context(question:str):\n",
    "    request=[question]\n",
    "    request_embedding = embed_text(request)\n",
    "    results = collection.query(\n",
    "        query_embeddings=request_embedding,\n",
    "        n_results=3,    \n",
    "    )\n",
    "    context = \"\\n##\".join([text for text in results['documents'][0]])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3da469-c694-4a97-b2bc-90219cb406e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question= \"Explain the formula used for training the LLaVa model\"\n",
    "output = llm_prediction(question)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
